{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qty_dot_url  qty_hyphen_url  qty_underline_url  qty_slash_url  \\\n",
      "0            3               0                  0              1   \n",
      "1            5               0                  1              3   \n",
      "2            2               0                  0              1   \n",
      "3            4               0                  2              5   \n",
      "4            2               0                  0              0   \n",
      "\n",
      "   qty_questionmark_url  qty_equal_url  qty_at_url  qty_and_url  \\\n",
      "0                     0              0           0            0   \n",
      "1                     0              3           0            2   \n",
      "2                     0              0           0            0   \n",
      "3                     0              0           0            0   \n",
      "4                     0              0           0            0   \n",
      "\n",
      "   qty_exclamation_url  qty_space_url  ...  qty_ip_resolved  qty_nameservers  \\\n",
      "0                    0              0  ...                1                2   \n",
      "1                    0              0  ...                1                2   \n",
      "2                    0              0  ...                1                2   \n",
      "3                    0              0  ...                1                2   \n",
      "4                    0              0  ...                1                2   \n",
      "\n",
      "   qty_mx_servers  ttl_hostname  tls_ssl_certificate  qty_redirects  \\\n",
      "0               0           892                    0              0   \n",
      "1               1          9540                    1              0   \n",
      "2               3           589                    1              0   \n",
      "3               0           292                    1              0   \n",
      "4               1          3597                    0              1   \n",
      "\n",
      "   url_google_index  domain_google_index  url_shortened  phishing  \n",
      "0                 0                    0              0         1  \n",
      "1                 0                    0              0         1  \n",
      "2                 0                    0              0         0  \n",
      "3                 0                    0              0         1  \n",
      "4                 0                    0              0         0  \n",
      "\n",
      "[5 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = 'dataset_full.csv'\n",
    "\n",
    "# Import the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of each variable with CLASS_LABEL:\n",
      "qty_slash_directory           0.746454\n",
      "qty_questionmark_file         0.745644\n",
      "qty_dollar_file               0.745644\n",
      "qty_hashtag_file              0.745644\n",
      "qty_slash_file                0.745644\n",
      "qty_hashtag_directory         0.745644\n",
      "qty_questionmark_directory    0.745644\n",
      "qty_at_file                   0.745509\n",
      "qty_exclamation_file          0.745059\n",
      "qty_and_file                  0.744860\n",
      "qty_tilde_file                0.744657\n",
      "qty_equal_file                0.743692\n",
      "qty_space_file                0.742749\n",
      "qty_comma_file                0.742364\n",
      "qty_comma_directory           0.742251\n",
      "qty_exclamation_directory     0.742187\n",
      "qty_space_directory           0.740026\n",
      "qty_tilde_directory           0.739571\n",
      "qty_equal_directory           0.737922\n",
      "qty_plus_file                 0.735140\n",
      "qty_dollar_directory          0.734923\n",
      "qty_dot_file                  0.733008\n",
      "qty_plus_directory            0.732842\n",
      "qty_and_directory             0.702265\n",
      "qty_slash_url                 0.699061\n",
      "qty_dot_directory             0.690271\n",
      "qty_asterisk_file             0.684798\n",
      "qty_at_directory              0.682272\n",
      "qty_asterisk_directory        0.651520\n",
      "qty_underline_file            0.636585\n",
      "qty_underline_directory       0.623106\n",
      "directory_length              0.525694\n",
      "qty_hyphen_file               0.493509\n",
      "length_url                    0.449771\n",
      "qty_hyphen_directory          0.449308\n",
      "qty_hashtag_params            0.356418\n",
      "qty_tilde_params              0.356328\n",
      "qty_space_params              0.355979\n",
      "qty_asterisk_params           0.355961\n",
      "qty_dollar_params             0.354275\n",
      "qty_exclamation_params        0.351800\n",
      "qty_comma_params              0.348753\n",
      "tld_present_params            0.345977\n",
      "qty_at_params                 0.343501\n",
      "qty_plus_params               0.343492\n",
      "qty_questionmark_params       0.343155\n",
      "qty_params                    0.303360\n",
      "qty_equal_params              0.294491\n",
      "qty_equal_url                 0.262597\n",
      "qty_underline_params          0.259413\n",
      "file_length                   0.255057\n",
      "qty_and_params                0.254607\n",
      "qty_percent_file              0.254090\n",
      "qty_percent_directory         0.251584\n",
      "qty_slash_params              0.249949\n",
      "qty_dot_params                0.245891\n",
      "qty_hyphen_params             0.234990\n",
      "params_length                 0.232992\n",
      "qty_tld_url                   0.214407\n",
      "qty_hyphen_url                0.200382\n",
      "qty_and_url                   0.191967\n",
      "qty_underline_url             0.191151\n",
      "email_in_url                  0.187812\n",
      "qty_dot_url                   0.171128\n",
      "qty_percent_params            0.122710\n",
      "qty_hyphen_domain             0.117456\n",
      "qty_questionmark_url          0.111694\n",
      "qty_vowels_domain             0.111333\n",
      "qty_at_url                    0.108708\n",
      "url_shortened                 0.099250\n",
      "asn_ip                        0.091654\n",
      "domain_in_ip                  0.058101\n",
      "qty_tilde_url                 0.051042\n",
      "time_response                 0.033274\n",
      "qty_exclamation_url           0.032796\n",
      "qty_comma_url                 0.028606\n",
      "qty_dollar_url                0.026142\n",
      "qty_percent_url               0.025766\n",
      "qty_asterisk_url              0.020681\n",
      "qty_space_url                 0.019224\n",
      "qty_hashtag_url               0.011327\n",
      "domain_length                 0.009736\n",
      "qty_plus_url                  0.008043\n",
      "qty_at_domain                 0.004621\n",
      "server_client_domain          0.002855\n",
      "domain_google_index           0.000794\n",
      "qty_underline_domain         -0.004514\n",
      "domain_spf                   -0.005582\n",
      "url_google_index             -0.008986\n",
      "ttl_hostname                 -0.010740\n",
      "qty_ip_resolved              -0.029340\n",
      "tls_ssl_certificate          -0.036249\n",
      "qty_redirects                -0.061775\n",
      "qty_nameservers              -0.066336\n",
      "qty_mx_servers               -0.079580\n",
      "time_domain_expiration       -0.165915\n",
      "qty_dot_domain               -0.260480\n",
      "time_domain_activation       -0.441875\n",
      "qty_slash_domain                   NaN\n",
      "qty_questionmark_domain            NaN\n",
      "qty_equal_domain                   NaN\n",
      "qty_and_domain                     NaN\n",
      "qty_exclamation_domain             NaN\n",
      "qty_space_domain                   NaN\n",
      "qty_tilde_domain                   NaN\n",
      "qty_comma_domain                   NaN\n",
      "qty_plus_domain                    NaN\n",
      "qty_asterisk_domain                NaN\n",
      "qty_hashtag_domain                 NaN\n",
      "qty_dollar_domain                  NaN\n",
      "qty_percent_domain                 NaN\n",
      "Name: phishing, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Get the correlation of each feature with the target variable (CLASS_LABEL)\n",
    "class_label_corr = correlation_matrix['phishing'].drop('phishing').sort_values(ascending=False)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Display the sorted correlations\n",
    "print(\"Correlation of each variable with CLASS_LABEL:\")\n",
    "print(class_label_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hear is a drop statement for every single column besides the phishing one\n",
    "#use this as a template and in another notebook paste it and remove the columns you plan on using\n",
    "\"\"\"\n",
    "df = df.drop(columns=[\n",
    "    'qty_slash_directory', 'qty_questionmark_file', 'qty_dollar_file', 'qty_hashtag_file', \n",
    "    'qty_slash_file', 'qty_hashtag_directory', 'qty_questionmark_directory', 'qty_at_file', \n",
    "    'qty_exclamation_file', 'qty_and_file', 'qty_tilde_file', 'qty_equal_file', \n",
    "    'qty_space_file', 'qty_comma_file', 'qty_comma_directory', 'qty_exclamation_directory', \n",
    "    'qty_space_directory', 'qty_tilde_directory', 'qty_equal_directory', 'qty_plus_file', \n",
    "    'qty_dollar_directory', 'qty_dot_file', 'qty_plus_directory', 'qty_and_directory', \n",
    "    'qty_slash_url', 'qty_dot_directory', 'qty_asterisk_file', 'qty_at_directory', \n",
    "    'qty_asterisk_directory', 'qty_underline_file', 'qty_underline_directory', \n",
    "    'directory_length', 'qty_hyphen_file', 'length_url', 'qty_hyphen_directory', \n",
    "    'qty_hashtag_params', 'qty_tilde_params', 'qty_space_params', 'qty_asterisk_params', \n",
    "    'qty_dollar_params', 'qty_exclamation_params', 'qty_comma_params', \n",
    "    'tld_present_params', 'qty_at_params', 'qty_plus_params', 'qty_questionmark_params', \n",
    "    'qty_params', 'qty_equal_params', 'qty_equal_url', 'qty_underline_params', \n",
    "    'file_length', 'qty_and_params', 'qty_percent_file', 'qty_percent_directory', \n",
    "    'qty_slash_params', 'qty_dot_params', 'qty_hyphen_params', 'params_length', \n",
    "    'qty_tld_url', 'qty_hyphen_url', 'qty_and_url', 'qty_underline_url', 'email_in_url', \n",
    "    'qty_dot_url', 'qty_percent_params', 'qty_hyphen_domain', 'qty_questionmark_url', \n",
    "    'qty_vowels_domain', 'qty_at_url', 'url_shortened', 'asn_ip', 'domain_in_ip', \n",
    "    'qty_tilde_url', 'time_response', 'qty_exclamation_url', 'qty_comma_url', \n",
    "    'qty_dollar_url', 'qty_percent_url', 'qty_asterisk_url', 'qty_space_url', \n",
    "    'qty_hashtag_url', 'domain_length', 'qty_plus_url', 'qty_at_domain', \n",
    "    'server_client_domain', 'domain_google_index', 'qty_underline_domain', \n",
    "    'domain_spf', 'url_google_index', 'ttl_hostname', 'qty_ip_resolved', \n",
    "    'tls_ssl_certificate', 'qty_redirects', 'qty_nameservers', 'qty_mx_servers', \n",
    "    'time_domain_expiration', 'qty_dot_domain', 'time_domain_activation', \n",
    "    'qty_slash_domain', 'qty_questionmark_domain', 'qty_equal_domain', 'qty_and_domain', \n",
    "    'qty_exclamation_domain', 'qty_space_domain', 'qty_tilde_domain', \n",
    "    'qty_comma_domain', 'qty_plus_domain', 'qty_asterisk_domain', 'qty_hashtag_domain', \n",
    "    'qty_dollar_domain', 'qty_percent_domain'\n",
    "], axis=1)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the working cell which will be modified to select which colums will be dropped\n",
    "#will need to execute cells above after modifying to bring back in columns that were dropped \n",
    "df = df.drop(columns=[\n",
    "    'qty_slash_directory', 'qty_questionmark_file', 'qty_dollar_file', 'qty_hashtag_file', \n",
    "    'qty_slash_file', 'qty_hashtag_directory', 'qty_questionmark_directory', 'qty_at_file', \n",
    "    'qty_exclamation_file', 'qty_and_file', 'qty_tilde_file', 'qty_equal_file', \n",
    "    'qty_space_file', 'qty_comma_file', 'qty_comma_directory', 'qty_exclamation_directory', \n",
    "    'qty_space_directory', 'qty_tilde_directory', 'qty_equal_directory', 'qty_plus_file', \n",
    "    'qty_dollar_directory', 'qty_dot_file', 'qty_plus_directory', 'qty_and_directory', \n",
    "    'qty_slash_url', 'qty_dot_directory', 'qty_asterisk_file', 'qty_at_directory', \n",
    "    'qty_asterisk_directory', 'qty_underline_file', 'qty_underline_directory', \n",
    "    'directory_length', 'qty_hyphen_file', 'length_url', 'qty_hyphen_directory', \n",
    "    'qty_hashtag_params', 'qty_tilde_params', 'qty_space_params', 'qty_asterisk_params', \n",
    "    'qty_dollar_params', 'qty_exclamation_params', 'qty_comma_params', \n",
    "    'tld_present_params', 'qty_at_params', 'qty_plus_params', 'qty_questionmark_params', \n",
    "    'qty_params', 'qty_equal_params', 'qty_equal_url', 'qty_underline_params', \n",
    "    'file_length', 'qty_and_params', 'qty_percent_file', 'qty_percent_directory', \n",
    "    'qty_slash_params', 'qty_dot_params', 'qty_hyphen_params', 'params_length', \n",
    "    'qty_tld_url', 'qty_hyphen_url', 'qty_and_url', 'qty_underline_url', 'email_in_url', \n",
    "    'qty_dot_url', 'qty_percent_params', 'qty_hyphen_domain', 'qty_questionmark_url', \n",
    "    'qty_vowels_domain', 'qty_at_url', 'url_shortened', 'asn_ip', 'domain_in_ip', \n",
    "    'qty_tilde_url', 'time_response', 'qty_exclamation_url', 'qty_comma_url', \n",
    "    'qty_dollar_url', 'qty_percent_url', 'qty_asterisk_url', 'qty_space_url', \n",
    "    'qty_hashtag_url', 'domain_length', 'qty_plus_url', 'qty_at_domain', \n",
    "    'server_client_domain', 'domain_google_index', 'qty_underline_domain', \n",
    "    'domain_spf', 'url_google_index', 'ttl_hostname', 'qty_ip_resolved', \n",
    "    'tls_ssl_certificate', 'qty_redirects', 'qty_nameservers', 'qty_mx_servers', \n",
    "    'time_domain_expiration', 'qty_dot_domain', 'time_domain_activation', \n",
    "    'qty_slash_domain', 'qty_questionmark_domain', 'qty_equal_domain', 'qty_and_domain', \n",
    "    'qty_exclamation_domain', 'qty_space_domain', 'qty_tilde_domain', \n",
    "    'qty_comma_domain', 'qty_plus_domain', 'qty_asterisk_domain', 'qty_hashtag_domain', \n",
    "    'qty_dollar_domain', 'qty_percent_domain'\n",
    "], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the following code to save a model after running it\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(clf, 'random_forest_classifier.pkl')\n",
    "\n",
    "#use the following code to load an existing model \n",
    "\n",
    "clfRF_loaded = joblib.load('random_forest_classifier.pkl')\n",
    "\n",
    "# Predict for a single new entry\n",
    "new_entry = [/* feature values for the new entry */]  # Replace with the new entry's features\n",
    "prediction = clfRF_loaded.predict([new_entry])\n",
    "\n",
    "print(f'Predicted class for the new entry: {prediction}')\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.34%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop('phishing', axis=1)  # Features\n",
    "y = df['phishing']               # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the decision tree on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Initialize the Logistic Regression Classifier\n",
    "clfLOG = LogisticRegression(random_state=42, max_iter=2000)  # Increase max_iter if needed\n",
    "\n",
    "# Train the logistic regression model on the training data\n",
    "clfLOG.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clfLOG.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.07%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize the Neural Network Classifier\n",
    "clfFFNN = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "# Train the neural network on the training data\n",
    "clfFFNN.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clfFFNN.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Initialize the Naive Bayes Classifier\n",
    "clfGB = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes model on the training data\n",
    "clfGB.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clfGB.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clfRF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the random forest on the training data\n",
    "clfRF.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clfRF.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Accuracy on scoring dataset (part 2): 96.46%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the part 1 and part 2 datasets\n",
    "part1_path = 'Phishing_Legitimate_new_part2.csv'\n",
    "part2_path = 'Phishing_Legitimate_new_part1.csv'\n",
    "\n",
    "df_train = pd.read_csv(part1_path)  # Part 1 used for training and testing\n",
    "df_score = pd.read_csv(part2_path)  # Part 2 used for scoring\n",
    "\n",
    "# Separate the features and target variable for part 1 (training set)\n",
    "X_train = df_train.drop('CLASS_LABEL', axis=1)\n",
    "y_train = df_train['CLASS_LABEL']\n",
    "\n",
    "# Separate the features and target variable for part 2 (scoring set)\n",
    "X_score = df_score.drop('CLASS_LABEL', axis=1)\n",
    "y_score = df_score['CLASS_LABEL']\n",
    "\n",
    "# Initialize a Decision Tree with GridSearch for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found by GridSearch\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "\n",
    "# Predict on the part 2 (scoring set)\n",
    "y_pred_score = clf.predict(X_score)\n",
    "\n",
    "# Calculate the accuracy of the model on the scoring set (part 2)\n",
    "score_accuracy = accuracy_score(y_score, y_pred_score)\n",
    "print(f'Accuracy on scoring dataset (part 2): {score_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSVs saved as Phishing_Legitimate_new_part1.csv and Phishing_Legitimate_new_part2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the part 1 and part 2 datasets\n",
    "part1_path = 'Phishing_Legitimate_part1.csv'\n",
    "part2_path = 'Phishing_Legitimate_part2.csv'\n",
    "\n",
    "df_part1 = pd.read_csv(part1_path)  # Part 1\n",
    "df_part2 = pd.read_csv(part2_path)  # Part 2\n",
    "\n",
    "# Concatenate both parts into one DataFrame\n",
    "df_combined = pd.concat([df_part1, df_part2])\n",
    "\n",
    "# Shuffle the combined DataFrame randomly\n",
    "df_combined = shuffle(df_combined, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the combined DataFrame back into two equal parts\n",
    "half_size = len(df_combined) // 2\n",
    "df_new_part1 = df_combined.iloc[:half_size]\n",
    "df_new_part2 = df_combined.iloc[half_size:]\n",
    "\n",
    "# Save the shuffled datasets back to CSV files\n",
    "new_part1_path = 'Phishing_Legitimate_new_part1.csv'\n",
    "new_part2_path = 'Phishing_Legitimate_new_part2.csv'\n",
    "\n",
    "df_new_part1.to_csv(new_part1_path, index=False)\n",
    "df_new_part2.to_csv(new_part2_path, index=False)\n",
    "\n",
    "print(f\"New CSVs saved as {new_part1_path} and {new_part2_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Accuracy on scoring dataset (part 2): 96.66%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the part 1 and part 2 datasets\n",
    "part1_path = 'Phishing_Legitimate_new_part1.csv'\n",
    "part2_path = 'Phishing_Legitimate_new_part2.csv'\n",
    "\n",
    "df_train = pd.read_csv(part1_path)  # Part 1 used for training and testing\n",
    "df_score = pd.read_csv(part2_path)  # Part 2 used for scoring\n",
    "\n",
    "# Separate the features and target variable for part 1 (training set)\n",
    "X_train = df_train.drop('CLASS_LABEL', axis=1)\n",
    "y_train = df_train['CLASS_LABEL']\n",
    "\n",
    "# Separate the features and target variable for part 2 (scoring set)\n",
    "X_score = df_score.drop('CLASS_LABEL', axis=1)\n",
    "y_score = df_score['CLASS_LABEL']\n",
    "\n",
    "# Initialize a Decision Tree with GridSearch for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found by GridSearch\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "\n",
    "# Predict on the part 2 (scoring set)\n",
    "y_pred_score = clf.predict(X_score)\n",
    "\n",
    "# Calculate the accuracy of the model on the scoring set (part 2)\n",
    "score_accuracy = accuracy_score(y_score, y_pred_score)\n",
    "print(f'Accuracy on scoring dataset (part 2): {score_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import pandas as pd
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant

# Step 1: Load the dataset (replace with your actual dataset path)
data = pd.read_csv('dataset_full.csv')

# Step 2: Define the dependent variable (y) and independent variables (X)
y = data['phishing']  # Replace 'phishing' with your actual target column name
X = data.drop(columns=['phishing'])  # Drop the target column from independent variables

# Step 3: Add constant for the intercept term
X = add_constant(X)

# Step 4: Stepwise selection function
def stepwise_selection(X, y, threshold_in=0.05, threshold_out=0.10):
    """
    Perform stepwise regression (both forward and backward) based on p-value.
    """
    initial_features = X.columns.tolist()
    included = list(initial_features)
    
    while True:
        # Forward step: Add the best feature
        excluded = list(set(initial_features) - set(included))
        new_pval = pd.Series(index=excluded, dtype=float)
        
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(X[included + [new_column]])).fit()
            new_pval[new_column] = model.pvalues[new_column]
        
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
        
        # Backward step: Remove the least significant feature
        model = sm.OLS(y, sm.add_constant(X[included])).fit()
        pvalues = model.pvalues.iloc[1:]  # Exclude constant
        worst_pval = pvalues.max()
        
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
        
        # Break if no features are added or removed
        if best_pval >= threshold_in and worst_pval <= threshold_out:
            break
    
    return included

# Step 5: Run the stepwise selection to identify relevant columns
selected_columns = stepwise_selection(X, y)
print("Selected columns:", selected_columns)

# Step 6: Fit a model with the selected columns
final_model = sm.OLS(y, sm.add_constant(X[selected_columns])).fit()

# Step 7: Display the summary of the final model
print(final_model.summary())

